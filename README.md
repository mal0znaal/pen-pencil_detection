# Тестовое задание "Детекция карандашей и ручек"

Цель: обучить и сравнить несколько моделей YOLO для детекции двух классов — **ручка** (`pen`) и **карандаш** (`pencil`) — на своих данных, а затем проверить качество на отдельном тестовом видео (и в реальном времени).

---

## 1. Постановка задачи

Техническое задание:

- Записать **два видео по ~30 секунд** с ручками и карандашами на столе:
  - первое видео — для формирования обучающего датасета;
  - второе — для финальной проверки модели.
- Выбрать **60 кадров** из первого видео:
  - 50 изображений в `train`;
  - 10 изображений в `val`.
- Разметить изображения:
- Обучить несколько моделей YOLO на GPU.
- Оценить метрики и ответить:
  - какая модель лучше;
  - как она ведёт себя на втором видео;
  - попробовать запустить модель в real-time.

---

## 2. Данные

### 2.1 Сбор

Было записано два видео:

- **video_train.mp4** — ~30 секунд, 3 ручки и 2 карандаша на столе, разные ракурсы и движения камеры. На некоторых кадрах предметы частично (или полностью) пропадают из зоны выдимости
- **video_test.mp4** — второе видео в отличающихся условиях:
  - немного другое расположение предметов;
  - частично другие фоны/углы.

### 2.2 Разметка

Разметка выполнялась в сервисе **Roboflow**, для ускорения этого этапа применялась модель **SAM3** для автоматической разметки. После работы **SAM3** каждое изображение проверялось и корректировалось вручную. Ссылка на датасет: https://app.roboflow.com/testproject-64hot/pen_penicl1-4izcj/1

Разделение на выборки:
- **train** — 50 изображений;
- **val** — 10 изображений.

---

## 3. Обучение модели

### 3.1 Первый эксперимент

После получение готового набора данных, была обучена модель **YOLO12s** на 50 эпохах. Все гиперпараметры обучения стандартные, кроме:  
- **Batch_size** выбирался автоматически, чтобы занимать **75%** VRAM
- **imgsz** = 640. Изображения сжимались до 640 пикселей по ширине с сохранением пропорций.

Цель - получить базовую модель без специального тюнинга и аугментаций а после понять, как предобученная YOLO12s справляется с задачей на данном датасете. 

| Class  | Precision (P) | Recall (R) | mAP@0.5 | mAP@0.5:0.95 |   F1   |
|--------|---------------|------------|---------|--------------|--------|
| all    | 0.996         | 0.985      | 0.995   | 0.869        | 0.9908 |
| pen    | 1.000         | 0.971      | 0.995   | 0.903        | ------ |
| pencil | 0.993         | 1.000      | 0.995   | 0.835        | ------ |

Результаты на вылидационной выборке оказались хорошими, однако для точной оценки результата стоит проверить как модель покажет себя в боевых условиях (второе видео, записанное для тестирования).  
И с этим возникают проблемы: Модель хорошо выделяет `bound boxes`, но ошибается в классификации. Один определенный карандаш ошибочно определяется за ручку.

#### Демонстрация работы модели до аугментаций



<div align="center">  


https://github.com/user-attachments/assets/d4ab74ed-2bb3-4b00-9717-99f59a80c7bb


</div>

<p align="center">
  <em>Видео: ДО аугментации, ~6&nbsp;МБ</em>
</p>

#### Гипотеза 1: конкуренция классов по уверенности (conf / NMS)

Посмотрим в чем проблема, если посмотреть видео подробно - то видно, что в какой-то момент модель все же верно классифицирует объект (к регрессионной способности определять ограничивающие рамки вопросов не возникает).  
Возможно, **conf** для классификации как `ручка` чуть выше чем для `карандаш`, и после срабатывания NMS мы не видим результат.  
Для проверки гипотезы очень сильно понизим порог NMS, в надежде увидить в той же области `bound box`, классифицированный как `карандаш`

![neg_case2](https://github.com/user-attachments/assets/5b998333-cc88-4240-86e0-d7e67a8e3cc1)

Результат: 
- модель **вообще не генерирует** детекций класса `карандаш` на этом объекте.
- проблема не в NMS и похожих уровнях **conf**, а в том, что модель **не воспринимает этот объект как карандаш в принципе**.

#### Гипотеза 2: Проблема с углами обзора
Но так как модель отлично определяет рамки -> делаем вывод, что модель еще не научилась различать ручки и карандаши.  
Это сильно зависит от **ориентации** и  **ракурса** съемки. 

Чтобы проверить эту гипотезу - повернем прошлое фото на 90°:
![neg_case](https://github.com/user-attachments/assets/cb3a1d0e-59da-4c13-b045-178f1a6dfaba)
И вау, модель распознала карандаш как `карандаш`, но ошиблась с ручкой)).

Результат: 
- модель сильно чувствительна к ориентации и ракурсу и переобучилась на ограниченном наборе из обучающей выборки.

### 3.2 Обучение новой модели с осознанием проблемы

Решим возникшую проблему 2 способами:
1. Добавим в обучающую выборку еще **10** изображений `проблемного` карандаша с различных ракурсов. Новый датасет: https://app.roboflow.com/testproject-64hot/pen_penicl1-4izcj/3
2. Применим дополнительные аугментации и обучим модель снова, на **дополненном** датасете:

```
    degrees=30.0,   # поворот ±30°
    scale=0.5,      # изменение масштаба
    shear=10.0,     # имитация съемки с разных углов
    fliplr=0.5,     # горизонтальные отражения
```

| Class  | Precision (P) | Recall (R) | mAP@0.5 | mAP@0.5:0.95 |   F1   |
|--------|---------------|------------|---------|--------------|--------|
| all    | 0.900         | 0.889      | 0.993   | 0.93         | 0.8946 |
| pen    | 0.980         | 1.000      | 0.995   | 0.995        | ------ |
| pencil | 0.820         | 0.778      | 0.865   | 0.865        | ------ |

Результаты на валидационном наборе стали чуть хуже, но что же с работой модели в реальных (тестовых) условиях? Из видео ниже заметно, что результат стал гораздо **лучше**. Классифицирующая способность модели стала только лучше.  
В целом, можно сказать, что модель стала **устойчивее**.

#### Демонстрация работы модели после аугментаций



<div align="center">  




https://github.com/user-attachments/assets/b3097126-eabd-4963-95e1-493ddac986a5




</div>

<p align="center">
  <em>Видео: ПОСЛЕ аугментации, ~6&nbsp;МБ</em>
</p>


### 3.3 Сравнение разных моделей

Далее сравним различные модели YOLO, учитывая полученные на прошлых шагах знания:

- `YOLO12n` — nano, самая лёгкая и быстрая;
- `YOLO12s` — small;
- `YOLO12m` — medium;
- `YOLO11s` — альтернативная линейка small;
- `RT-DETR-L` — детектор на основе трансформеров (альтернативная YOLO архитектура, входит в фреймворк `ultralytics`).

Все модели обучались на одном и том же датасете с одинаковыми настройками (количество эпох, размер изображения, аугментации).  
Для каждой модели отдельно считались метрики на валидационной выборке:

| Model    | Precision (P) | Recall (R) |   F1   | mAP@0.5 | mAP@0.5:0.95 |
|----------|--------------:|-----------:|-------:|--------:|-------------:|
| yolo12s  | 0.9660        | 0.9722     | 0.9691 | 0.9802  | 0.6656       |
| yolo12n  | 1.0000        | 0.5370     | 0.6987 | 0.9083  | 0.6590       |
| yolo12m  | 0.7449        | 0.9213     | 0.8238 | 0.8288  | 0.4135       |
| yolo11s  | 0.9157        | 0.9595     | 0.9371 | 0.9784  | 0.6907       |
| rtdetr-l | 0.9667        | 0.9591     | 0.9629 | 0.9603  | 0.5777       |

Интерпретация:
- **yolo12s** оказалась лучшей моделью. Точность и полнота (а также F1 мера) у нее оказались лучше всех остальных.
- **yolo12n** оказалось очень "осторожной" моделью, которая почти не делает `ложных` предсказаний. Однако, `recall` на классе `pencil` оказался необычайно низким (0.315).
- **yolo12m** не оправдала ожиданий от количества своих параметров. Вероятно для достижения лучшего качества требуется больше эпох + гораздо больше данных.
- **yolo11s** очень похожа на **yolo12s**, однако с результатами несколько хуже. В данном случае новая версия модели оказалась лучше предыдущей.
- **RT-DETR** показал очень близкий к лидеру результат (если оценивать по F1 мере).


Сравним три лучшие модели:

| Model     | Layers | Params (M) | GFLOPs | Precision (P) | Recall (R) |   F1   | mAP@0.5 | mAP@0.5:0.95 |
|-----------|--------|-----------:|-------:|---------------|-----------:|-------:|--------:|-------------:|
| YOLO12s   | 159    |      9.23  |  21.2  | 0.966         | 0.972      | 0.9690 | 0.980   | 0.667        |
| YOLO11s   | 100    |      9.41  |  21.3  | 0.916         | 0.959      | 0.9370 | 0.978   | 0.690        |
| RT-DETR-L | 302    |     31.99  | 103.4  | 0.966         | 0.959      | 0.9625 | 0.960   | 0.579        |

В итоге, **YOLO12s** очевидно лучшая модель. **RT-DETR** мог бы быть хорош, но не на этих данных - тут его возможности не раскрылись.  

**Итог:**  
В качестве основной модели для решения задачи детекции ручек и карандашей была выбрана модель **YOLO12s**


### 3.4 Дообучение лучшей модели:

Было решено дообучить лучшую модель на большом количестве эпох с небольшим `learning rate`. Это должно чуть-чуть поднять качество работы модели.  
Для этого были дополнены следующие гиперпараметры, а аугментации оставлены как в шаге 3.2:
```
        epochs=200,
        optimizer="AdamW",
        lr0=1e-4,       # стартовый lr 
        lrf=0.005,      # финальный lr = lr0 * 0.005
        cos_lr=True,    
        warmup_epochs=5.0,
        patience=30,    # электричество тоже денег стоит
```
В итоге, на 69 шаге модель достигла лучшего результата:

| Class   | P      | R      | mAP@0.5 | mAP@0.5:0.95 |   F1    |   
|---------|--------|--------|---------|--------------|---------|
| all     | 0.995  | 1.000  | 0.995   | 0.780        | 0.9976  |
| pen     | 0.991  | 1.000  | 0.995   | 0.834        | ------  |
| pencil  | 1.000  | 1.000  | 0.995   | 0.727        | ------  |

Эти метрики  лучше исходных значений для базовой YOLO12s.
В итоге аккуратный long fine-tune с маленьким lr действительно улучшил и качество классификации, и точность локализации.
---

## 4. Финальный инференс


<div align="center">  


https://github.com/user-attachments/assets/a3b4149b-f664-401f-bc2a-50482a036822


</div>

<p align="center">
  <em>Видео: ФИНАЛЬНАЯ модель, ~6&nbsp;МБ</em>
</p>
